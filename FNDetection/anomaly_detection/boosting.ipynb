{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORT SECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import  StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING SET LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28000 28000\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "data=[]\n",
    "labels=[]\n",
    "with open(\"../FakeNewsNet/fake_with_feat.csv\") as f:\n",
    "    reader = csv.reader(f)\n",
    "\n",
    "    conto=0\n",
    "    for row in reader:\n",
    "        l=[row[el] for el in range(len(row)-1)]\n",
    "        data.append(l)\n",
    "        labels.append(0)\n",
    "        if(int(float(row[-1]))==-1):\n",
    "            conto+=1\n",
    "            if(conto==14000):\n",
    "                break\n",
    "\n",
    "with open(\"../FakeNewsNet/real_with_feat.csv\") as f:\n",
    "    reader = csv.reader(f)\n",
    "\n",
    "    conto=0\n",
    "    for row in reader:\n",
    "        l=[row[el] for el in range(len(row)-1)]       \n",
    "        data.append(l)\n",
    "        labels.append(int(float(row[-1])))\n",
    "        if(int(float(row[-1]))==1):\n",
    "            conto+=1\n",
    "            if(conto==14000):\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "print(len(data), len(labels))\n",
    "data = np.array(data, dtype='f')\n",
    "labels = np.array(labels, dtype=int)\n",
    "\n",
    "data, labels = shuffle(data, labels, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING AND VALIDATION SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19600 8400 19600 8400\n"
     ]
    }
   ],
   "source": [
    "#cretae module to load a dataset and prepare train and validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(data, labels, random_state=42, test_size=0.3)\n",
    "print(len(X_train), len(X_val), len(y_train), len(y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST SET LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2518, 57) (2518,)\n"
     ]
    }
   ],
   "source": [
    "data1=[]\n",
    "labels1=[]\n",
    "with open(\"../test.csv\") as f:\n",
    "        reader = csv.reader(f)\n",
    "\n",
    "        conto=0\n",
    "        for row in reader:\n",
    "            if len(row) ==0:\n",
    "                continue\n",
    "            \n",
    "            data1.append(row[:-1])\n",
    "            if(int(float(row[-1]))==-1):\n",
    "              labels1.append(0)\n",
    "            else:\n",
    "              labels1.append(int(float(row[-1])))\n",
    "            if(int(float(row[-1]))==1):\n",
    "                conto+=1\n",
    "                if(conto==1259):\n",
    "                    break\n",
    "\n",
    "data1 = np.array(data1, dtype='f')\n",
    "labels1 = np.array(labels1, dtype=int)\n",
    "\n",
    "data1, labels1 = shuffle(data1, labels1, random_state=42)\n",
    "print(data1.shape, labels1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBOOST TRAINING FRAMEWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5832\n"
     ]
    }
   ],
   "source": [
    "n_estimators=np.arange(100, 200, 50)\n",
    "learning_rate=np.arange(0.1, 1, 0.1)\n",
    "base_score=np.arange(0.1, 0.5, 0.1)\n",
    "max_depth=np.arange(1, 10, 1)\n",
    "subsample=np.arange(0.1, 1, 0.1)\n",
    "\n",
    "dizionario={}\n",
    "j=0\n",
    "for n in n_estimators:\n",
    "    for lr in learning_rate:\n",
    "      for b in base_score:\n",
    "        for m in max_depth:\n",
    "            for samp in subsample:\n",
    "              dizionario[j]=(n, lr, b, m, samp)\n",
    "              j+=1\n",
    "print(len(dizionario))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "accuracy1=[]\n",
    "val_accuracy=[]\n",
    "val_covs=[]\n",
    "covs1=[]\n",
    "\n",
    "i=0\n",
    "for n in n_estimators:\n",
    "    for lr in learning_rate:\n",
    "      for b in base_score:\n",
    "        for m in max_depth:\n",
    "            for samp in subsample:\n",
    "              model = xgb.XGBClassifier(n_estimators = n, learning_rate=lr, base_score=b, max_depth=m, subsample=samp).fit(X_train, y_train)\n",
    "              val_preds=model.predict(X_val)\n",
    "              preds1=model.predict(data1)\n",
    "\n",
    "              val_accuracy.append(balanced_accuracy_score(y_val, val_preds))\n",
    "              accuracy1.append(balanced_accuracy_score(labels1, preds1))\n",
    "              val_covs.append(confusion_matrix(y_val, val_preds))\n",
    "              covs1.append(confusion_matrix(labels1, preds1))\n",
    "              i+=1\n",
    "              if(i%10==0):\n",
    "                print(i)\n",
    "\n",
    "val_covs=np.array(val_covs)\n",
    "val_accuracy=np.array(val_accuracy)\n",
    "covs1=np.array(covs1)\n",
    "accuracy1=np.array(accuracy1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEST BOOSTER SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(accuracy1)):\n",
    "    if(accuracy1[i]>0.6):\n",
    "        if(val_accuracy[i]>0.7):\n",
    "            print(accuracy1[i], val_accuracy[i], i)\n",
    "            print(covs1[i], val_covs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RECOMPUTE PERFORMANCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6386020651310564\n",
      "0.7499922902214611\n",
      "[[707 552]\n",
      " [358 901]]\n",
      "[[2341 1851]\n",
      " [ 246 3962]]\n"
     ]
    }
   ],
   "source": [
    "#model = xgb.XGBClassifier(n_estimators = 1000, learning_rate=0.1, base_score=0.1, max_depth=7, subsample=1.5, colsample_bytree=0.8).fit(X_train, y_train)\n",
    "model = xgb.XGBClassifier(n_estimators=100,base_score=0.9,learning_rate= 0.01,max_depth=15).fit(X_train, y_train)\n",
    "#preds = model.predict(data)\n",
    "val_preds=model.predict(X_val)\n",
    "preds1= model.predict(data1)\n",
    "\n",
    "for i in range(len(preds1)):\n",
    "    preds1[i] =0 if preds1[i]<0.5 else 1\n",
    "\n",
    "for i in range(len(val_preds)):\n",
    "    val_preds[i] = 0 if val_preds[i]<0.5 else 1\n",
    "\n",
    "#print(balanced_accuracy_score(labels, preds))\n",
    "print(balanced_accuracy_score(labels1, preds1))\n",
    "print(balanced_accuracy_score(y_val, val_preds))\n",
    "\n",
    "#print(confusion_matrix(labels, preds))\n",
    "print(confusion_matrix(labels1, preds1))\n",
    "print(confusion_matrix(y_val, val_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL SAVING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "\n",
    "filepath=\"../models/xgbmodel\"\n",
    "pk.dump(model, open(filepath, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEATURE IMPROTANCE PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# plot feature importance\n",
    "xgb.plot_importance(model)\n",
    "plt.gcf().set_size_inches(20, 20)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c89dac2000cd39b6c6afdbb40384525553e908a214fa5e3caa43ad6708c415f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
